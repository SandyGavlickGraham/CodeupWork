{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling\n",
    "Do your work for these exercises in either a notebook or a python script named model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_id</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "      <th>embarked_encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8750</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>848</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>484</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>91.0792</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>609</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153.4625</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     passenger_id  survived  pclass     sex   age  sibsp  parch      fare  \\\n",
       "821           821         1       3    male  27.0      0      0    8.6625   \n",
       "422           422         0       3    male  29.0      0      0    7.8750   \n",
       "848           848         0       2    male  28.0      0      1   33.0000   \n",
       "484           484         1       1    male  25.0      1      0   91.0792   \n",
       "609           609         1       1  female  40.0      0      0  153.4625   \n",
       "\n",
       "    embarked   class  embark_town  alone  embarked_encode  \n",
       "821        S   Third  Southampton      1                2  \n",
       "422        S   Third  Southampton      1                2  \n",
       "848        S  Second  Southampton      0                2  \n",
       "484        C   First    Cherbourg      0                0  \n",
       "609        S   First  Southampton      1                2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from acquire import get_titanic_data\n",
    "from prepare import prep_titanic_data\n",
    "\n",
    "df = get_titanic_data()\n",
    "df = prep_titanic_data(df)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "1. Fit the logistic regression classifier to your training sample and transform, i.e. make predictions on the training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in the `age` column.\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.9000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>146.5208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass   age      fare  sibsp  parch\n",
       "60        3  22.0    7.2292      0      0\n",
       "348       3   3.0   15.9000      1      1\n",
       "606       3  30.0    7.8958      0      0\n",
       "195       1  58.0  146.5208      0      0\n",
       "56        2  21.0   10.5000      0      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df[['survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# 1. make the thing\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 2. fit the thing\n",
    "scaler.fit(X_train[['age', 'fare']])\n",
    "\n",
    "# 3. use the thing\n",
    "X_train[['age', 'fare']] = scaler.transform(X_train[['age', 'fare']])\n",
    "X_test[['age', 'fare']] = scaler.transform(X_test[['age', 'fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "#### Create the logistic regression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(C=1, class_weight={1:2}, random_state = 123, solver='saga')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight={1: 2}, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=123, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the coefficients and intercept of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[-1.06836414 -1.9727291   0.79910148 -0.27300495  0.40904858]]\n",
      "\n",
      "Intercept: \n",
      " [3.3184823]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit.coef_)\n",
    "print()\n",
    "print('Intercept: \\n', logit.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluate your in-sample results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate whether or not a passenger would survive, using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the probability of a passenger surviving, using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logit.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_train['prediction'] = logit.predict(X_train[['pclass','age','fare','sibsp','parch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6933867735470942"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train.survived == X_train.prediction).sum() / y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6933867735470942"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.score(X_train[['pclass','age','fare','sibsp','parch']], y_train.survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "#### Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.69\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train.drop(columns='prediction'), y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190 103]\n",
      " [ 50 156]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred -</th>\n",
       "      <th>Pred +</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual -</th>\n",
       "      <td>190</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual +</th>\n",
       "      <td>50</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pred -  Pred +\n",
       "Actual -     190     103\n",
       "Actual +      50     156"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(confusion_matrix(y_train.survived, X_train.prediction),\n",
    "             columns=['Pred -', 'Pred +'], index=['Actual -', 'Actual +'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Precision, Recall, F1-score, and Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71       293\n",
      "           1       0.60      0.76      0.67       206\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       499\n",
      "   macro avg       0.70      0.70      0.69       499\n",
      "weighted avg       0.71      0.69      0.70       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n",
    "#### Compute the accuracy of the model when run on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on test set: 0.67\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on test set: {:.2f}'\n",
    "     .format(logit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred -</th>\n",
       "      <th>Pred +</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual -</th>\n",
       "      <td>190</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual +</th>\n",
       "      <td>50</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pred -  Pred +\n",
       "Actual -     190     103\n",
       "Actual +      50     156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(confusion_matrix(y_train.survived, X_train.prediction),\n",
    "             columns=['Pred -', 'Pred +'], index=['Actual -', 'Actual +'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative =  190\n",
      "False Positive =  103\n",
      "False Negative =  50\n",
      "True Positive =  156\n",
      "Total =  499\n"
     ]
    }
   ],
   "source": [
    "TN = df['Pred -'][0] # 190\n",
    "FP = df['Pred +'][0] #103\n",
    "FN = df['Pred -'][1] # 50\n",
    "TP = df['Pred +'][1] # 156\n",
    "total = TN + FP + FN + TP\n",
    "\n",
    "print('True Negative = ', TN)\n",
    "print('False Positive = ', FP)\n",
    "print('False Negative = ', FN)\n",
    "print('True Positive = ', TP)\n",
    "print('Total = ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6933867735470942\n"
     ]
    }
   ],
   "source": [
    "# Accuracy = # correct / total \n",
    "#          = (true positive + true negative) / total\n",
    "accuracy = (TP + TN) / total\n",
    "print('Accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall =  0.7572815533980582\n"
     ]
    }
   ],
   "source": [
    "# Recall = Sensitivity\n",
    "#      = true positive rate \n",
    "#      = true positive / (true positive + false negative) \n",
    "recall = TP / (TP + FN)\n",
    "print('Recall = ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity =  0.3515358361774744\n"
     ]
    }
   ],
   "source": [
    "# Specificity = false positive rate\n",
    "#      = false positive / (false positive + true negative)\n",
    "specificity = FP / (FP + TN)\n",
    "print('Specificity = ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative Rate =  0.6484641638225256\n"
     ]
    }
   ],
   "source": [
    "# true negative rate = true negative / (true negative + false positive)\n",
    "trueneg = TN / (TN + FP)\n",
    "print('True Negative Rate = ', trueneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Rate =  0.24271844660194175\n"
     ]
    }
   ],
   "source": [
    "# false negative rate = false negative / (false negaitve + true positive)\n",
    "falseneg = FN / (FN + TP)\n",
    "print('False Negative Rate = ', falseneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision =  0.6023166023166023\n"
     ]
    }
   ],
   "source": [
    "# precision = true positive / (true positive + false positive)\n",
    "precision = TP / (TP + FP)\n",
    "print('Precision = ', precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score is  0.6797990778573303\n"
     ]
    }
   ],
   "source": [
    "f1 = (precision + recall) / 2\n",
    "print('f1-score is ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293 people died and 206 people lived.\n"
     ]
    }
   ],
   "source": [
    "died = TN + FP\n",
    "lived = TP + FN\n",
    "print(died, 'people died and', lived, 'people lived.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Look in the scikit-learn documentation to research the solver parameter. What is your best option(s) for the particular problem you are trying to solve and the data to be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.linear_model.LogisticRegression(\n",
    "    penalty=’l2’, \n",
    "    dual=False, \n",
    "    tol=0.0001, \n",
    "    C=1.0, \n",
    "    fit_intercept=True, \n",
    "    intercept_scaling=1, \n",
    "    class_weight=None, \n",
    "    random_state=None, \n",
    "    solver=’warn’, \n",
    "    max_iter=100, \n",
    "    multi_class=’warn’, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    n_jobs=None)\n",
    "    \n",
    "solver : str, {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default: ‘liblinear’.\n",
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "- For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
    "- For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "\n",
    "‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas ‘liblinear’ and ‘saga’ handle L1 penalty.\n",
    "Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.\n",
    "\n",
    "We just want the default!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run through steps 2-4 using another solver (from question 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.69\n",
      "[[190 103]\n",
      " [ 50 156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71       293\n",
      "           1       0.60      0.76      0.67       206\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       499\n",
      "   macro avg       0.70      0.70      0.69       499\n",
      "weighted avg       0.71      0.69      0.70       499\n",
      "\n",
      "True Negative =  190\n",
      "False Positive =  103\n",
      "False Negative =  50\n",
      "True Positive =  156\n",
      "Total =  499\n",
      "Accuracy =  0.6933867735470942\n",
      "Recall =  0.7572815533980582\n",
      "Specificity =  0.3515358361774744\n",
      "True Negative Rate =  0.6484641638225256\n",
      "False Negative Rate =  0.24271844660194175\n",
      "Precision =  0.6023166023166023\n",
      "f1-score is  0.6797990778573303\n",
      "293 people died and 206 people lived.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# for saga solver:\n",
    "X_train = []\n",
    "df = get_titanic_data()\n",
    "df = prep_titanic_data(df)\n",
    "# Handle missing values in the `age` column.\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df[['survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# 1. make the thing\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 2. fit the thing\n",
    "scaler.fit(X_train[['age', 'fare']])\n",
    "\n",
    "# 3. use the thing\n",
    "X_train[['age', 'fare']] = scaler.transform(X_train[['age', 'fare']])\n",
    "X_test[['age', 'fare']] = scaler.transform(X_test[['age', 'fare']])\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(C=1, class_weight={1:2}, random_state = 123, solver='saga')\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_train)\n",
    "y_pred_proba = logit.predict_proba(X_train)\n",
    "X_train['prediction'] = logit.predict(X_train[['pclass','age','fare','sibsp','parch']])\n",
    "# (y_train.survived == X_train.prediction).sum() / y_train.shape[0]\n",
    "# logit.score(X_train[['pclass','age','fare','sibsp','parch']], y_train.survived)\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train.drop(columns='prediction'), y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "df = pd.DataFrame(confusion_matrix(y_train.survived, X_train.prediction),\n",
    "             columns=['Pred -', 'Pred +'], index=['Actual -', 'Actual +'])\n",
    "print(classification_report(y_train, y_pred))\n",
    "TN = df['Pred -'][0] # 190\n",
    "FP = df['Pred +'][0] #103\n",
    "FN = df['Pred -'][1] # 50\n",
    "TP = df['Pred +'][1] # 156\n",
    "total = TN + FP + FN + TP\n",
    "\n",
    "print('True Negative = ', TN)\n",
    "print('False Positive = ', FP)\n",
    "print('False Negative = ', FN)\n",
    "print('True Positive = ', TP)\n",
    "print('Total = ', total)\n",
    "\n",
    "# Accuracy = # correct / total \n",
    "#          = (true positive + true negative) / total\n",
    "accuracy = (TP + TN) / total\n",
    "print('Accuracy = ', accuracy)\n",
    "\n",
    "# Recall = Sensitivity\n",
    "#      = true positive rate \n",
    "#      = true positive / (true positive + false negative) \n",
    "recall = TP / (TP + FN)\n",
    "print('Recall = ', recall)\n",
    "\n",
    "# Specificity = false positive rate\n",
    "#      = false positive / (false positive + true negative)\n",
    "specificity = FP / (FP + TN)\n",
    "print('Specificity = ', specificity)\n",
    "\n",
    "# true negative rate = true negative / (true negative + false positive)\n",
    "trueneg = TN / (TN + FP)\n",
    "print('True Negative Rate = ', trueneg)\n",
    "\n",
    "# false negative rate = false negative / (false negaitve + true positive)\n",
    "falseneg = FN / (FN + TP)\n",
    "print('False Negative Rate = ', falseneg)\n",
    "\n",
    "# precision = true positive / (true positive + false positive)\n",
    "precision = TP / (TP + FP)\n",
    "print('Precision = ', precision)\n",
    "\n",
    "f1 = (precision + recall) / 2\n",
    "print('f1-score is ', f1)\n",
    "\n",
    "died = TN + FP\n",
    "lived = TP + FN\n",
    "print(died, 'people died and', lived, 'people lived.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.69\n",
      "[[187 106]\n",
      " [ 51 155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.64      0.70       293\n",
      "           1       0.59      0.75      0.66       206\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       499\n",
      "   macro avg       0.69      0.70      0.68       499\n",
      "weighted avg       0.71      0.69      0.69       499\n",
      "\n",
      "True Negative =  187\n",
      "False Positive =  106\n",
      "False Negative =  51\n",
      "True Positive =  155\n",
      "Total =  499\n",
      "Accuracy =  0.685370741482966\n",
      "Recall =  0.7524271844660194\n",
      "Specificity =  0.36177474402730375\n",
      "True Negative Rate =  0.6382252559726962\n",
      "False Negative Rate =  0.24757281553398058\n",
      "Precision =  0.5938697318007663\n",
      "f1-score is  0.6731484581333929\n",
      "293 people died and 206 people lived.\n"
     ]
    }
   ],
   "source": [
    "# liblinear solver\n",
    "X_train = []\n",
    "df = get_titanic_data()\n",
    "df = prep_titanic_data(df)\n",
    "# Handle missing values in the `age` column.\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df[['survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# 1. make the thing\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 2. fit the thing\n",
    "scaler.fit(X_train[['age', 'fare']])\n",
    "\n",
    "# 3. use the thing\n",
    "X_train[['age', 'fare']] = scaler.transform(X_train[['age', 'fare']])\n",
    "X_test[['age', 'fare']] = scaler.transform(X_test[['age', 'fare']])\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(C=1, class_weight={1:2}, random_state = 123, solver='liblinear')\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_train)\n",
    "y_pred_proba = logit.predict_proba(X_train)\n",
    "X_train['prediction'] = logit.predict(X_train[['pclass','age','fare','sibsp','parch']])\n",
    "# (y_train.survived == X_train.prediction).sum() / y_train.shape[0]\n",
    "# logit.score(X_train[['pclass','age','fare','sibsp','parch']], y_train.survived)\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train.drop(columns='prediction'), y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "df = pd.DataFrame(confusion_matrix(y_train.survived, X_train.prediction),\n",
    "             columns=['Pred -', 'Pred +'], index=['Actual -', 'Actual +'])\n",
    "print(classification_report(y_train, y_pred))\n",
    "TN = df['Pred -'][0] # 190\n",
    "FP = df['Pred +'][0] #103\n",
    "FN = df['Pred -'][1] # 50\n",
    "TP = df['Pred +'][1] # 156\n",
    "total = TN + FP + FN + TP\n",
    "\n",
    "print('True Negative = ', TN)\n",
    "print('False Positive = ', FP)\n",
    "print('False Negative = ', FN)\n",
    "print('True Positive = ', TP)\n",
    "print('Total = ', total)\n",
    "\n",
    "# Accuracy = # correct / total \n",
    "#          = (true positive + true negative) / total\n",
    "accuracy = (TP + TN) / total\n",
    "print('Accuracy = ', accuracy)\n",
    "\n",
    "# Recall = Sensitivity\n",
    "#      = true positive rate \n",
    "#      = true positive / (true positive + false negative) \n",
    "recall = TP / (TP + FN)\n",
    "print('Recall = ', recall)\n",
    "\n",
    "# Specificity = false positive rate\n",
    "#      = false positive / (false positive + true negative)\n",
    "specificity = FP / (FP + TN)\n",
    "print('Specificity = ', specificity)\n",
    "\n",
    "# true negative rate = true negative / (true negative + false positive)\n",
    "trueneg = TN / (TN + FP)\n",
    "print('True Negative Rate = ', trueneg)\n",
    "\n",
    "# false negative rate = false negative / (false negaitve + true positive)\n",
    "falseneg = FN / (FN + TP)\n",
    "print('False Negative Rate = ', falseneg)\n",
    "\n",
    "# precision = true positive / (true positive + false positive)\n",
    "precision = TP / (TP + FP)\n",
    "print('Precision = ', precision)\n",
    "\n",
    "f1 = (precision + recall) / 2\n",
    "print('f1-score is ', f1)\n",
    "\n",
    "died = TN + FP\n",
    "lived = TP + FN\n",
    "print(died, 'people died and', lived, 'people lived.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.69\n",
      "[[190 103]\n",
      " [ 50 156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71       293\n",
      "           1       0.60      0.76      0.67       206\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       499\n",
      "   macro avg       0.70      0.70      0.69       499\n",
      "weighted avg       0.71      0.69      0.70       499\n",
      "\n",
      "True Negative =  190\n",
      "False Positive =  103\n",
      "False Negative =  50\n",
      "True Positive =  156\n",
      "Total =  499\n",
      "Accuracy =  0.6933867735470942\n",
      "Recall =  0.7572815533980582\n",
      "Specificity =  0.3515358361774744\n",
      "True Negative Rate =  0.6484641638225256\n",
      "False Negative Rate =  0.24271844660194175\n",
      "Precision =  0.6023166023166023\n",
      "f1-score is  0.6797990778573303\n",
      "293 people died and 206 people lived.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# newton-cg solver\n",
    "X_train = []\n",
    "df = get_titanic_data()\n",
    "df = prep_titanic_data(df)\n",
    "# Handle missing values in the `age` column.\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df[['survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# 1. make the thing\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 2. fit the thing\n",
    "scaler.fit(X_train[['age', 'fare']])\n",
    "\n",
    "# 3. use the thing\n",
    "X_train[['age', 'fare']] = scaler.transform(X_train[['age', 'fare']])\n",
    "X_test[['age', 'fare']] = scaler.transform(X_test[['age', 'fare']])\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(C=1, class_weight={1:2}, random_state = 123, solver='newton-cg')\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_train)\n",
    "y_pred_proba = logit.predict_proba(X_train)\n",
    "X_train['prediction'] = logit.predict(X_train[['pclass','age','fare','sibsp','parch']])\n",
    "# (y_train.survived == X_train.prediction).sum() / y_train.shape[0]\n",
    "# logit.score(X_train[['pclass','age','fare','sibsp','parch']], y_train.survived)\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train.drop(columns='prediction'), y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "df = pd.DataFrame(confusion_matrix(y_train.survived, X_train.prediction),\n",
    "             columns=['Pred -', 'Pred +'], index=['Actual -', 'Actual +'])\n",
    "print(classification_report(y_train, y_pred))\n",
    "TN = df['Pred -'][0] # 190\n",
    "FP = df['Pred +'][0] #103\n",
    "FN = df['Pred -'][1] # 50\n",
    "TP = df['Pred +'][1] # 156\n",
    "total = TN + FP + FN + TP\n",
    "\n",
    "print('True Negative = ', TN)\n",
    "print('False Positive = ', FP)\n",
    "print('False Negative = ', FN)\n",
    "print('True Positive = ', TP)\n",
    "print('Total = ', total)\n",
    "\n",
    "# Accuracy = # correct / total \n",
    "#          = (true positive + true negative) / total\n",
    "accuracy = (TP + TN) / total\n",
    "print('Accuracy = ', accuracy)\n",
    "\n",
    "# Recall = Sensitivity\n",
    "#      = true positive rate \n",
    "#      = true positive / (true positive + false negative) \n",
    "recall = TP / (TP + FN)\n",
    "print('Recall = ', recall)\n",
    "\n",
    "# Specificity = false positive rate\n",
    "#      = false positive / (false positive + true negative)\n",
    "specificity = FP / (FP + TN)\n",
    "print('Specificity = ', specificity)\n",
    "\n",
    "# true negative rate = true negative / (true negative + false positive)\n",
    "trueneg = TN / (TN + FP)\n",
    "print('True Negative Rate = ', trueneg)\n",
    "\n",
    "# false negative rate = false negative / (false negaitve + true positive)\n",
    "falseneg = FN / (FN + TP)\n",
    "print('False Negative Rate = ', falseneg)\n",
    "\n",
    "# precision = true positive / (true positive + false positive)\n",
    "precision = TP / (TP + FP)\n",
    "print('Precision = ', precision)\n",
    "\n",
    "f1 = (precision + recall) / 2\n",
    "print('f1-score is ', f1)\n",
    "\n",
    "died = TN + FP\n",
    "lived = TP + FN\n",
    "print(died, 'people died and', lived, 'people lived.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.69\n",
      "[[190 103]\n",
      " [ 50 156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71       293\n",
      "           1       0.60      0.76      0.67       206\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       499\n",
      "   macro avg       0.70      0.70      0.69       499\n",
      "weighted avg       0.71      0.69      0.70       499\n",
      "\n",
      "True Negative =  190\n",
      "False Positive =  103\n",
      "False Negative =  50\n",
      "True Positive =  156\n",
      "Total =  499\n",
      "Accuracy =  0.6933867735470942\n",
      "Recall =  0.7572815533980582\n",
      "Specificity =  0.3515358361774744\n",
      "True Negative Rate =  0.6484641638225256\n",
      "False Negative Rate =  0.24271844660194175\n",
      "Precision =  0.6023166023166023\n",
      "f1-score is  0.6797990778573303\n",
      "293 people died and 206 people lived.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# sag solver\n",
    "X_train = []\n",
    "df = get_titanic_data()\n",
    "df = prep_titanic_data(df)\n",
    "# Handle missing values in the `age` column.\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df[['survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# 1. make the thing\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 2. fit the thing\n",
    "scaler.fit(X_train[['age', 'fare']])\n",
    "\n",
    "# 3. use the thing\n",
    "X_train[['age', 'fare']] = scaler.transform(X_train[['age', 'fare']])\n",
    "X_test[['age', 'fare']] = scaler.transform(X_test[['age', 'fare']])\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(C=1, class_weight={1:2}, random_state = 123, solver='sag')\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_train)\n",
    "y_pred_proba = logit.predict_proba(X_train)\n",
    "X_train['prediction'] = logit.predict(X_train[['pclass','age','fare','sibsp','parch']])\n",
    "# (y_train.survived == X_train.prediction).sum() / y_train.shape[0]\n",
    "# logit.score(X_train[['pclass','age','fare','sibsp','parch']], y_train.survived)\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train.drop(columns='prediction'), y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "df = pd.DataFrame(confusion_matrix(y_train.survived, X_train.prediction),\n",
    "             columns=['Pred -', 'Pred +'], index=['Actual -', 'Actual +'])\n",
    "print(classification_report(y_train, y_pred))\n",
    "TN = df['Pred -'][0] # 190\n",
    "FP = df['Pred +'][0] #103\n",
    "FN = df['Pred -'][1] # 50\n",
    "TP = df['Pred +'][1] # 156\n",
    "total = TN + FP + FN + TP\n",
    "\n",
    "print('True Negative = ', TN)\n",
    "print('False Positive = ', FP)\n",
    "print('False Negative = ', FN)\n",
    "print('True Positive = ', TP)\n",
    "print('Total = ', total)\n",
    "\n",
    "# Accuracy = # correct / total \n",
    "#          = (true positive + true negative) / total\n",
    "accuracy = (TP + TN) / total\n",
    "print('Accuracy = ', accuracy)\n",
    "\n",
    "# Recall = Sensitivity\n",
    "#      = true positive rate \n",
    "#      = true positive / (true positive + false negative) \n",
    "recall = TP / (TP + FN)\n",
    "print('Recall = ', recall)\n",
    "\n",
    "# Specificity = false positive rate\n",
    "#      = false positive / (false positive + true negative)\n",
    "specificity = FP / (FP + TN)\n",
    "print('Specificity = ', specificity)\n",
    "\n",
    "# true negative rate = true negative / (true negative + false positive)\n",
    "trueneg = TN / (TN + FP)\n",
    "print('True Negative Rate = ', trueneg)\n",
    "\n",
    "# false negative rate = false negative / (false negaitve + true positive)\n",
    "falseneg = FN / (FN + TP)\n",
    "print('False Negative Rate = ', falseneg)\n",
    "\n",
    "# precision = true positive / (true positive + false positive)\n",
    "precision = TP / (TP + FP)\n",
    "print('Precision = ', precision)\n",
    "\n",
    "f1 = (precision + recall) / 2\n",
    "print('f1-score is ', f1)\n",
    "\n",
    "died = TN + FP\n",
    "lived = TP + FN\n",
    "print(died, 'people died and', lived, 'people lived.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.69\n",
      "[[190 103]\n",
      " [ 50 156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71       293\n",
      "           1       0.60      0.76      0.67       206\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       499\n",
      "   macro avg       0.70      0.70      0.69       499\n",
      "weighted avg       0.71      0.69      0.70       499\n",
      "\n",
      "True Negative =  190\n",
      "False Positive =  103\n",
      "False Negative =  50\n",
      "True Positive =  156\n",
      "Total =  499\n",
      "Accuracy =  0.6933867735470942\n",
      "Recall =  0.7572815533980582\n",
      "Specificity =  0.3515358361774744\n",
      "True Negative Rate =  0.6484641638225256\n",
      "False Negative Rate =  0.24271844660194175\n",
      "Precision =  0.6023166023166023\n",
      "f1-score is  0.6797990778573303\n",
      "293 people died and 206 people lived.\n"
     ]
    }
   ],
   "source": [
    "# lbfgs solver\n",
    "X_train = []\n",
    "df = get_titanic_data()\n",
    "df = prep_titanic_data(df)\n",
    "# Handle missing values in the `age` column.\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df[['survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# 1. make the thing\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 2. fit the thing\n",
    "scaler.fit(X_train[['age', 'fare']])\n",
    "\n",
    "# 3. use the thing\n",
    "X_train[['age', 'fare']] = scaler.transform(X_train[['age', 'fare']])\n",
    "X_test[['age', 'fare']] = scaler.transform(X_test[['age', 'fare']])\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(C=1, class_weight={1:2}, random_state = 123, solver='lbfgs')\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_train)\n",
    "y_pred_proba = logit.predict_proba(X_train)\n",
    "X_train['prediction'] = logit.predict(X_train[['pclass','age','fare','sibsp','parch']])\n",
    "# (y_train.survived == X_train.prediction).sum() / y_train.shape[0]\n",
    "# logit.score(X_train[['pclass','age','fare','sibsp','parch']], y_train.survived)\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train.drop(columns='prediction'), y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "df = pd.DataFrame(confusion_matrix(y_train.survived, X_train.prediction),\n",
    "             columns=['Pred -', 'Pred +'], index=['Actual -', 'Actual +'])\n",
    "print(classification_report(y_train, y_pred))\n",
    "TN = df['Pred -'][0] # 190\n",
    "FP = df['Pred +'][0] #103\n",
    "FN = df['Pred -'][1] # 50\n",
    "TP = df['Pred +'][1] # 156\n",
    "total = TN + FP + FN + TP\n",
    "\n",
    "print('True Negative = ', TN)\n",
    "print('False Positive = ', FP)\n",
    "print('False Negative = ', FN)\n",
    "print('True Positive = ', TP)\n",
    "print('Total = ', total)\n",
    "\n",
    "# Accuracy = # correct / total \n",
    "#          = (true positive + true negative) / total\n",
    "accuracy = (TP + TN) / total\n",
    "print('Accuracy = ', accuracy)\n",
    "\n",
    "# Recall = Sensitivity\n",
    "#      = true positive rate \n",
    "#      = true positive / (true positive + false negative) \n",
    "recall = TP / (TP + FN)\n",
    "print('Recall = ', recall)\n",
    "\n",
    "# Specificity = false positive rate\n",
    "#      = false positive / (false positive + true negative)\n",
    "specificity = FP / (FP + TN)\n",
    "print('Specificity = ', specificity)\n",
    "\n",
    "# true negative rate = true negative / (true negative + false positive)\n",
    "trueneg = TN / (TN + FP)\n",
    "print('True Negative Rate = ', trueneg)\n",
    "\n",
    "# false negative rate = false negative / (false negaitve + true positive)\n",
    "falseneg = FN / (FN + TP)\n",
    "print('False Negative Rate = ', falseneg)\n",
    "\n",
    "# precision = true positive / (true positive + false positive)\n",
    "precision = TP / (TP + FP)\n",
    "print('Precision = ', precision)\n",
    "\n",
    "f1 = (precision + recall) / 2\n",
    "print('f1-score is ', f1)\n",
    "\n",
    "died = TN + FP\n",
    "lived = TP + FN\n",
    "print(died, 'people died and', lived, 'people lived.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Which performs better on your in-sample data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the same results for all of the solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Save the best model in logit_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight={1: 2}, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=123, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_fit = logit\n",
    "logit_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "1. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for features Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], dtype='object')\n",
      "[0.         0.         0.57027606 0.42972394]\n",
      "\n",
      "[[0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.5   0.5  ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.5   0.5  ]\n",
      " [0.    0.    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "df = data('iris')\n",
    "\n",
    "df.columns = [col.lower().replace('.', '_') for col in df]\n",
    "\n",
    "X = df.drop(['species'],axis=1)\n",
    "y = df[['species']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# for classificaiton you can change the algorithm as gini or entropy \n",
    "# (information gain).  Default is gini.\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=123)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"for features\", X_train.columns)\n",
    "print(clf.feature_importances_)\n",
    "print()\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "#print(y_pred[0:5]) # ['virginica' 'virginica' 'versicolor' 'setosa' 'setosa']\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_train)\n",
    "print(y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluate your in-sample results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[32,  0,  0],\n",
       "       [ 0, 40,  0],\n",
       "       [ 0,  2, 31]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            setosa  versicolor  virginica\n",
       "setosa          32           0          0\n",
       "versicolor       0          40          0\n",
       "virginica        0           2         31"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(y_train.species.unique())\n",
    "y_train.species.value_counts()\n",
    "\n",
    "labels = sorted(y_train.species.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_train, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        32\n",
      "  versicolor       0.95      1.00      0.98        40\n",
      "   virginica       1.00      0.94      0.97        33\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       105\n",
      "   macro avg       0.98      0.98      0.98       105\n",
      "weighted avg       0.98      0.98      0.98       105\n",
      "\n",
      "Accuracy of Decision Tree classifier on test set: 0.93\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iris_decision_tree2.pdf'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## need to install graphviz to anaconda\n",
    "## example: \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# iris = load_iris()\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(iris.data, iris.target)\n",
    "\n",
    "import graphviz\n",
    "\n",
    "from graphviz import Graph\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph.render('iris_decision_tree2', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run through steps 2-4 using entropy as your measure of impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        32\n",
      "  versicolor       0.95      1.00      0.98        40\n",
      "   virginica       1.00      0.94      0.97        33\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       105\n",
      "   macro avg       0.98      0.98      0.98       105\n",
      "weighted avg       0.98      0.98      0.98       105\n",
      "\n",
      "Accuracy of Decision Tree classifier on test set: 0.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        32\n",
      "  versicolor       0.95      1.00      0.98        40\n",
      "   virginica       1.00      0.94      0.97        33\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       105\n",
      "   macro avg       0.98      0.98      0.98       105\n",
      "weighted avg       0.98      0.98      0.98       105\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            setosa  versicolor  virginica\n",
       "setosa          32           0          0\n",
       "versicolor       0          40          0\n",
       "virginica        0           2         31"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "df = data('iris')\n",
    "\n",
    "df.columns = [col.lower().replace('.', '_') for col in df]\n",
    "\n",
    "X = df.drop(['species'],axis=1)\n",
    "y = df[['species']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# for classificaiton you can change the algorithm as gini or entropy \n",
    "# (information gain).  Default is gini.\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=123)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "#print(y_pred[0:5]) # ['virginica' 'virginica' 'versicolor' 'setosa' 'setosa']\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "sorted(y_train.species.unique())\n",
    "y_train.species.value_counts()\n",
    "\n",
    "labels = sorted(y_train.species.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_train, y_pred), index=labels, columns=labels)\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_train, y_pred), index=labels, columns=labels)\n",
    "print(classification_report(y_train, y_pred))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iris_decision_tree2.pdf'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## need to install graphviz to anaconda\n",
    "## example: \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "\n",
    "import graphviz\n",
    "\n",
    "from graphviz import Graph\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph.render('iris_decision_tree2', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which performs better on your in-sample data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save the best model in tree_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_fit = clf\n",
    "tree_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "1. Fit the K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from acquire import get_titanic_data\n",
    "from prepare import prep_titanic_data\n",
    "\n",
    "df = prep_titanic_data(get_titanic_data())\n",
    "\n",
    "df.dropna(inplace=True) # handle missing age values\n",
    "\n",
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df[['survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# weights = ['uniform', 'density']\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "# choosing to be closest to five nearest neighbors\n",
    "# could weight features\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_train)\n",
    "\n",
    "y_pred_proba = knn.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.76\n",
      "[[239  54]\n",
      " [ 65 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80       293\n",
      "           1       0.72      0.68      0.70       206\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       499\n",
      "   macro avg       0.75      0.75      0.75       499\n",
      "weighted avg       0.76      0.76      0.76       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.76\n",
      "[[239  54]\n",
      " [ 65 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80       293\n",
      "           1       0.72      0.68      0.70       206\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       499\n",
      "   macro avg       0.75      0.75      0.75       499\n",
      "weighted avg       0.76      0.76      0.76       499\n",
      "\n",
      "Accuracy of KNN classifier on test set: 0.67\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))\n",
    "print('Accuracy of KNN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run through steps 1-3 setting k to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.71\n",
      "[[252  41]\n",
      " [103 103]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.86      0.78       293\n",
      "           1       0.72      0.50      0.59       206\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       499\n",
      "   macro avg       0.71      0.68      0.68       499\n",
      "weighted avg       0.71      0.71      0.70       499\n",
      "\n",
      "Accuracy of KNN classifier on test set: 0.70\n"
     ]
    }
   ],
   "source": [
    "# weights = ['uniform', 'density']\n",
    "knn = KNeighborsClassifier(n_neighbors=10, weights='uniform')\n",
    "# choosing to be closest to five nearest neighbors\n",
    "# could weight features\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_train)\n",
    "\n",
    "y_pred_proba = knn.predict_proba(X_train)\n",
    "\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "print('Accuracy of KNN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run through setps 1-3 setting k to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.71\n",
      "[[249  44]\n",
      " [100 106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.78       293\n",
      "           1       0.71      0.51      0.60       206\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       499\n",
      "   macro avg       0.71      0.68      0.69       499\n",
      "weighted avg       0.71      0.71      0.70       499\n",
      "\n",
      "Accuracy of KNN classifier on test set: 0.72\n"
     ]
    }
   ],
   "source": [
    "# weights = ['uniform', 'density']\n",
    "knn = KNeighborsClassifier(n_neighbors=20, weights='uniform')\n",
    "# choosing to be closest to five nearest neighbors\n",
    "# could weight features\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_train)\n",
    "\n",
    "y_pred_proba = knn.predict_proba(X_train)\n",
    "\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "print('Accuracy of KNN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbor mode with k = 5 was the best fit on my in-sample data with 76% accuracy, but the test data only yielded a 67% accuracy. The rest of the metrics look comparable. I guess I'll save k = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Save the best model in knn_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fit = KNeighborsClassifier(n_neighbors=5, weights='uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "1. Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nulls = \n",
      "passenger_id       0\n",
      "survived           0\n",
      "pclass             0\n",
      "sex                0\n",
      "age                0\n",
      "sibsp              0\n",
      "parch              0\n",
      "fare               0\n",
      "embarked           0\n",
      "class              0\n",
      "embark_town        0\n",
      "alone              0\n",
      "embarked_encode    0\n",
      "dtype: int64\n",
      "\n",
      "this shows gini-index, shows you the importance of each feature in order\n",
      "shows that fare is biggest indicator of survival\n",
      "for features ['pclass','age','fare','sibsp','parch']:\n",
      "[0.10439494 0.39013442 0.38148822 0.06701136 0.05697105]\n"
     ]
    }
   ],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from acquire import get_titanic_data\n",
    "from prepare import prep_titanic_data\n",
    "\n",
    "df = prep_titanic_data(get_titanic_data())\n",
    "\n",
    "# Handle missing age values\n",
    "df.dropna(inplace=True)\n",
    "print('number of nulls = ')\n",
    "print(df.isnull().sum())\n",
    "print()\n",
    "\n",
    "X = df[['pclass','age','fare','sibsp','parch']]\n",
    "y = df.survived\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# setting the random_state accordingly and \n",
    "# setting min_samples_leaf = 1 and max_depth = 20.\n",
    "rf = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=1,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=20, \n",
    "                            random_state=123)\n",
    "# min_samples_leaf is set to only 3 because dataset is small\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print('this shows gini-index, shows you the importance of each feature in order')\n",
    "print('shows that fare is biggest indicator of survival')\n",
    "print(\"for features ['pclass','age','fare','sibsp','parch']:\")\n",
    "print(rf.feature_importances_)\n",
    "\n",
    "\n",
    "y_pred = rf.predict(X_train)\n",
    "\n",
    "y_pred_proba = rf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest classifier on training set: 0.98\n",
      "\n",
      "[[291   2]\n",
      " [  6 200]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       293\n",
      "           1       0.99      0.97      0.98       206\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       499\n",
      "   macro avg       0.98      0.98      0.98       499\n",
      "weighted avg       0.98      0.98      0.98       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "     .format(rf.score(X_train, y_train)))\n",
    "print()\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "# y_train is rows\n",
    "# y_pred is columns\n",
    "\n",
    "# these numbers are from lesson... need to be changed...\n",
    "# 248 - pred died, died     |45 -  pred to survive, died\n",
    "# 79 - pred died, survived  |127 - pred to survive, survived\n",
    "\n",
    "# accuracy = (248 + 127) / (248 + 79 + 45 + 127)\n",
    "# recall of surviving = sensitivity = 127 / (79 + 127)\n",
    "# recall of not surviving = specificity = 248 / (248 + 5)\n",
    "# precision of surviving = 127 / (45 + 127)\n",
    "# precision of not surviving = 248 / (248 + 79)\n",
    "# false negative = 79 / (248 + 79)\n",
    "\n",
    "print()\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest classifier on TRAIN datat: 0.98\n",
      "\n",
      "[[291   2]\n",
      " [  6 200]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       293\n",
      "           1       0.99      0.97      0.98       206\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       499\n",
      "   macro avg       0.98      0.98      0.98       499\n",
      "weighted avg       0.98      0.98      0.98       499\n",
      "\n",
      "Accuracy of output when model is run on TEST data:\n",
      "0.7116279069767442\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of random forest classifier on TRAIN datat: {:.2f}'\n",
    "     .format(rf.score(X_train, y_train)))\n",
    "print()\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print()\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "print('Accuracy of output when model is run on TEST data:')\n",
    "print(rf.score(X_test, y_test))\n",
    "# print()\n",
    "# print(confusion_matrix(y_train, y_pred))\n",
    "# print()\n",
    "# print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run through steps increasing your min_samples_leaf to 5 and decreasing your max_depth to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this shows gini-index, shows you the importance of each feature in order\n",
      "shows that fare is biggest indicator of survival\n",
      "for features ['pclass','age','fare','sibsp','parch']:\n",
      "[0.31756957 0.13479889 0.39019831 0.07086815 0.08656508]\n",
      "\n",
      "Accuracy of random forest classifier on TRAIN datat: 0.75\n",
      "\n",
      "[[247  46]\n",
      " [ 79 127]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80       293\n",
      "           1       0.73      0.62      0.67       206\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       499\n",
      "   macro avg       0.75      0.73      0.73       499\n",
      "weighted avg       0.75      0.75      0.75       499\n",
      "\n",
      "Accuracy of output when model is run on TEST data:\n",
      "0.7441860465116279\n"
     ]
    }
   ],
   "source": [
    "# setting the random_state accordingly and \n",
    "# setting min_samples_leaf = 5 and max_depth = 3.\n",
    "rf = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=5,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=3, \n",
    "                            random_state=123)\n",
    "# min_samples_leaf is set to only 3 because dataset is small\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print('this shows gini-index, shows you the importance of each feature in order')\n",
    "print('shows that fare is biggest indicator of survival')\n",
    "print(\"for features ['pclass','age','fare','sibsp','parch']:\")\n",
    "print(rf.feature_importances_)\n",
    "print()\n",
    "\n",
    "y_pred = rf.predict(X_train)\n",
    "\n",
    "y_pred_proba = rf.predict_proba(X_train)\n",
    "\n",
    "# y_train is rows\n",
    "# y_pred is columns\n",
    "\n",
    "# these numbers are from lesson... need to be changed...\n",
    "# 248 - pred died, died     |45 -  pred to survive, died\n",
    "# 79 - pred died, survived  |127 - pred to survive, survived\n",
    "\n",
    "# accuracy = (248 + 127) / (248 + 79 + 45 + 127)\n",
    "# recall of surviving = sensitivity = 127 / (79 + 127)\n",
    "# recall of not surviving = specificity = 248 / (248 + 5)\n",
    "# precision of surviving = 127 / (45 + 127)\n",
    "# precision of not surviving = 248 / (248 + 79)\n",
    "# false negative = 79 / (248 + 79)\n",
    "\n",
    "print('Accuracy of random forest classifier on TRAIN datat: {:.2f}'\n",
    "     .format(rf.score(X_train, y_train)))\n",
    "print()\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print()\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "print('Accuracy of output when model is run on TEST data:')\n",
    "print(rf.score(X_test, y_test))\n",
    "# print()\n",
    "# print(confusion_matrix(y_train, y_pred))\n",
    "# print()\n",
    "# print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first random forest classifier setting min_samples_leaf = 1 and max_depth = 20 gave MUCH better in-sample results with an accuracy of 98%, but its accuracy on the test data was only 71% suggesting the model was overfit for the data. So I'll go with the second classifier with the min_samples_leaf to 5 and decreasing your max_depth to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save the best model in forest_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_fit = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=5,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=3, \n",
    "                            random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor:\n",
    "\n",
    "Accuracy of KNN classifier on training set: 0.76\n",
    "[[239  54]\n",
    " [ 65 141]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.82      0.80       293\n",
    "           1       0.72      0.68      0.70       206\n",
    "\n",
    "   micro avg       0.76      0.76      0.76       499\n",
    "   macro avg       0.75      0.75      0.75       499\n",
    "weighted avg       0.76      0.76      0.76       499\n",
    "\n",
    "Accuracy of KNN classifier on test set: 0.67\n",
    "\n",
    "Random Forest Classifier:\n",
    "\n",
    "for features ['pclass','age','fare','sibsp','parch']:\n",
    "[0.31756957 0.13479889 0.39019831 0.07086815 0.08656508]\n",
    "\n",
    "Accuracy of random forest classifier on TRAIN datat: 0.75\n",
    "\n",
    "[[247  46]\n",
    " [ 79 127]]\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.76      0.84      0.80       293\n",
    "           1       0.73      0.62      0.67       206\n",
    "\n",
    "   micro avg       0.75      0.75      0.75       499\n",
    "   macro avg       0.75      0.73      0.73       499\n",
    "weighted avg       0.75      0.75      0.75       499\n",
    "\n",
    "Accuracy of output when model is run on TEST data:\n",
    "0.7441860465116279\n",
    "\n",
    "Going with K-Nearest Neighbor model setting k = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Once you have determined which algorithm (with metaparameters) performs the best, try reducing the number of features to the top 4 features in terms of information gained for each feature individually. That is, how close do we get to predicting accurately the survival with each feature?\n",
    "\n",
    "1. Compute the information gained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "108           7.3          2.9           6.3          1.8  virginica\n",
       "135           6.1          2.6           5.6          1.4  virginica\n",
       "146           6.7          3.0           5.2          2.3  virginica"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data('iris')\n",
    "\n",
    "df.columns = [col.lower().replace('.', '_') for col in df]\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.1</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4.4</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>5.3</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     petal_length  petal_width\n",
       "1             1.4          0.2\n",
       "2             1.4          0.2\n",
       "3             1.3          0.2\n",
       "4             1.5          0.2\n",
       "5             1.4          0.2\n",
       "6             1.7          0.4\n",
       "7             1.4          0.3\n",
       "8             1.5          0.2\n",
       "9             1.4          0.2\n",
       "10            1.5          0.1\n",
       "11            1.5          0.2\n",
       "12            1.6          0.2\n",
       "13            1.4          0.1\n",
       "14            1.1          0.1\n",
       "15            1.2          0.2\n",
       "16            1.5          0.4\n",
       "17            1.3          0.4\n",
       "18            1.4          0.3\n",
       "19            1.7          0.3\n",
       "20            1.5          0.3\n",
       "21            1.7          0.2\n",
       "22            1.5          0.4\n",
       "23            1.0          0.2\n",
       "24            1.7          0.5\n",
       "25            1.9          0.2\n",
       "26            1.6          0.2\n",
       "27            1.6          0.4\n",
       "28            1.5          0.2\n",
       "29            1.4          0.2\n",
       "30            1.6          0.2\n",
       "31            1.6          0.2\n",
       "32            1.5          0.4\n",
       "33            1.5          0.1\n",
       "34            1.4          0.2\n",
       "35            1.5          0.2\n",
       "36            1.2          0.2\n",
       "37            1.3          0.2\n",
       "38            1.4          0.1\n",
       "39            1.3          0.2\n",
       "40            1.5          0.2\n",
       "41            1.3          0.3\n",
       "42            1.3          0.3\n",
       "43            1.3          0.2\n",
       "44            1.6          0.6\n",
       "45            1.9          0.4\n",
       "46            1.4          0.3\n",
       "47            1.6          0.2\n",
       "48            1.4          0.2\n",
       "49            1.5          0.2\n",
       "50            1.4          0.2\n",
       "51            4.7          1.4\n",
       "52            4.5          1.5\n",
       "53            4.9          1.5\n",
       "54            4.0          1.3\n",
       "55            4.6          1.5\n",
       "56            4.5          1.3\n",
       "57            4.7          1.6\n",
       "58            3.3          1.0\n",
       "59            4.6          1.3\n",
       "60            3.9          1.4\n",
       "61            3.5          1.0\n",
       "62            4.2          1.5\n",
       "63            4.0          1.0\n",
       "64            4.7          1.4\n",
       "65            3.6          1.3\n",
       "66            4.4          1.4\n",
       "67            4.5          1.5\n",
       "68            4.1          1.0\n",
       "69            4.5          1.5\n",
       "70            3.9          1.1\n",
       "71            4.8          1.8\n",
       "72            4.0          1.3\n",
       "73            4.9          1.5\n",
       "74            4.7          1.2\n",
       "75            4.3          1.3\n",
       "76            4.4          1.4\n",
       "77            4.8          1.4\n",
       "78            5.0          1.7\n",
       "79            4.5          1.5\n",
       "80            3.5          1.0\n",
       "81            3.8          1.1\n",
       "82            3.7          1.0\n",
       "83            3.9          1.2\n",
       "84            5.1          1.6\n",
       "85            4.5          1.5\n",
       "86            4.5          1.6\n",
       "87            4.7          1.5\n",
       "88            4.4          1.3\n",
       "89            4.1          1.3\n",
       "90            4.0          1.3\n",
       "91            4.4          1.2\n",
       "92            4.6          1.4\n",
       "93            4.0          1.2\n",
       "94            3.3          1.0\n",
       "95            4.2          1.3\n",
       "96            4.2          1.2\n",
       "97            4.2          1.3\n",
       "98            4.3          1.3\n",
       "99            3.0          1.1\n",
       "100           4.1          1.3\n",
       "101           6.0          2.5\n",
       "102           5.1          1.9\n",
       "103           5.9          2.1\n",
       "104           5.6          1.8\n",
       "105           5.8          2.2\n",
       "106           6.6          2.1\n",
       "107           4.5          1.7\n",
       "108           6.3          1.8\n",
       "109           5.8          1.8\n",
       "110           6.1          2.5\n",
       "111           5.1          2.0\n",
       "112           5.3          1.9\n",
       "113           5.5          2.1\n",
       "114           5.0          2.0\n",
       "115           5.1          2.4\n",
       "116           5.3          2.3\n",
       "117           5.5          1.8\n",
       "118           6.7          2.2\n",
       "119           6.9          2.3\n",
       "120           5.0          1.5\n",
       "121           5.7          2.3\n",
       "122           4.9          2.0\n",
       "123           6.7          2.0\n",
       "124           4.9          1.8\n",
       "125           5.7          2.1\n",
       "126           6.0          1.8\n",
       "127           4.8          1.8\n",
       "128           4.9          1.8\n",
       "129           5.6          2.1\n",
       "130           5.8          1.6\n",
       "131           6.1          1.9\n",
       "132           6.4          2.0\n",
       "133           5.6          2.2\n",
       "134           5.1          1.5\n",
       "135           5.6          1.4\n",
       "136           6.1          2.3\n",
       "137           5.6          2.4\n",
       "138           5.5          1.8\n",
       "139           4.8          1.8\n",
       "140           5.4          2.1\n",
       "141           5.6          2.4\n",
       "142           5.1          2.3\n",
       "143           5.1          1.9\n",
       "144           5.9          2.3\n",
       "145           5.7          2.5\n",
       "146           5.2          2.3\n",
       "147           5.0          1.9\n",
       "148           5.2          2.0\n",
       "149           5.4          2.3\n",
       "150           5.1          1.8"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['species', 'sepal_length', 'sepal_width'],axis=1)\n",
    "y = df[['species']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for features Index(['petal_length', 'petal_width'], dtype='object')\n",
      "[0.57027606 0.42972394]\n",
      "\n",
      "[[0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.5   0.5  ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.    1.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [1.    0.    0.   ]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.975 0.025]\n",
      " [0.    0.5   0.5  ]\n",
      " [0.    0.    1.   ]]\n",
      "Accuracy of Decision Tree classifier on training set: 0.98\n",
      "[[32  0  0]\n",
      " [ 0 40  0]\n",
      " [ 0  2 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        32\n",
      "  versicolor       0.95      1.00      0.98        40\n",
      "   virginica       1.00      0.94      0.97        33\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       105\n",
      "   macro avg       0.98      0.98      0.98       105\n",
      "weighted avg       0.98      0.98      0.98       105\n",
      "\n",
      "Accuracy of Decision Tree classifier on test set: 0.93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'iris_decision_tree3.pdf'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123)\n",
    "\n",
    "# for classificaiton you can change the algorithm as gini or entropy \n",
    "# (information gain).  Default is gini.\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=123)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"for features\", X_train.columns)\n",
    "print(clf.feature_importances_)\n",
    "print()\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "#print(y_pred[0:5]) # ['virginica' 'virginica' 'versicolor' 'setosa' 'setosa']\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_train)\n",
    "print(y_pred_proba)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(cm)\n",
    "\n",
    "sorted(y_train.species.unique())\n",
    "y_train.species.value_counts()\n",
    "\n",
    "labels = sorted(y_train.species.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_train, y_pred), index=labels, columns=labels)\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "import graphviz\n",
    "\n",
    "from graphviz import Graph\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph.render('iris_decision_tree3', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new dataframe with top 4 features (train_df_reduced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_reduced = df.drop(['species'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the top performing algorithm with the metaparameters used in that model. Create the object, fit, transform on in-sample data, and evaluate the results. Compare your evaluation metrics with those from the original model (with all the features). Select the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run your final model on your out-of-sample dataframe (test_df). Evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Titanic Data\n",
    "    - Create a feature named who, this should be either man, woman, or child. How does including this feature effect your model's performance?\n",
    "    - Create a feature named adult_male that is either a 1 or a 0. How does this effect your model's predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iris Data\n",
    "    - Create features named petal_area and sepal_area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
