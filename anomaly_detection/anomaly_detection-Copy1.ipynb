{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "file name: anomaly_detection.py or anomaly_detection.ipynb\n",
    "\n",
    "#### Discrete data + probability\n",
    "Use basic probability to identify anomalous request methods. You will want to make sure the text is normalized in order to reduce the noise.\n",
    "\n",
    "#### Time series + EMA\n",
    "Discover users who are accessing our curriculum pages way beyond the end of their codeup time. What would the dataframe look like? Use time series method for detecting anomalies, like exponential moving average with %b.\n",
    "\n",
    "#### Clustering - DBSCAN\n",
    "Use dbscan to detect anomalies in other products from the customers dataset.\n",
    "\n",
    "Use dbscan to detect anomalies in number of bedrooms and finished square feet of property for the filtered dataset you used in the clustering project (single unit properties with a logerror)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# %%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection of Discrete Data using Probability\n",
    "## Discrete data + probability\n",
    "\n",
    "### Use basic probability to identify anomalous request methods. You will want to make sure the text is normalized in order to reduce the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from random import randint\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrangle Data\n",
    "##### Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['ip', 'timestamp', 'request_method', 'status', 'size',\n",
    "          'destination', 'request_agent']\n",
    "df_orig = pd.read_csv('http://python.zach.lol/access.log',          \n",
    "                 engine='python',\n",
    "                 header=None,\n",
    "                 index_col=False,\n",
    "                 names=colnames,\n",
    "                 sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n",
    "                 na_values='\"-\"',\n",
    "                 usecols=[0, 3, 4, 5, 6, 7, 8]\n",
    ")\n",
    "\n",
    "new = pd.DataFrame([[\"95.31.18.119\", \"[21/Apr/2019:10:02:41+0000]\", \n",
    "                     \"GET /api/v1/items/HTTP/1.1\", 200, 1153005, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"95.31.16.121\", \"[17/Apr/2019:19:36:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 301, 1005, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"97.105.15.120\", \"[18/Apr/2019:19:42:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 301, 2560, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"97.105.19.58\", \"[19/Apr/2019:19:42:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 200, 2056327, np.nan, \n",
    "                     \"python-requests/2.21.0\"]], columns=colnames)\n",
    "\n",
    "df = df_orig.append(new)\n",
    "df.timestamp = df.timestamp.str.replace(r'(\\[|\\])', '', regex=True)\n",
    "df.timestamp= pd.to_datetime(df.timestamp.str.replace(':', ' ', 1)) \n",
    "df = df.set_index('timestamp')\n",
    "for col in ['request_method', 'request_agent', 'destination']:\n",
    "    df[col] = df[col].str.replace('\"', '')\n",
    "\n",
    "df['request_method'] = df.request_method.str.replace(r'\\?page=[0-9]+', '', regex=True)\n",
    "\n",
    "df['size_mb'] = [n/1024/1024 for n in df['size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>request_method</th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>destination</th>\n",
       "      <th>request_agent</th>\n",
       "      <th>size_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:42</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>512495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>0.488753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:42</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/items HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>3561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:44</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>510103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>0.486472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:46</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>510003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>0.486377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16 19:34:48</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>511963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>0.488246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17 12:55:14</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>510166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>0.486532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-21 10:02:41</th>\n",
       "      <td>95.31.18.119</td>\n",
       "      <td>GET /api/v1/items/HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>1153005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>1.099591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17 19:36:41</th>\n",
       "      <td>95.31.16.121</td>\n",
       "      <td>GET /api/v1/sales/HTTP/1.1</td>\n",
       "      <td>301</td>\n",
       "      <td>1005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>0.000958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-18 19:42:41</th>\n",
       "      <td>97.105.15.120</td>\n",
       "      <td>GET /api/v1/sales/HTTP/1.1</td>\n",
       "      <td>301</td>\n",
       "      <td>2560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>0.002441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-19 19:42:41</th>\n",
       "      <td>97.105.19.58</td>\n",
       "      <td>GET /api/v1/sales/HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>2056327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python-requests/2.21.0</td>\n",
       "      <td>1.961066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ip              request_method  status  \\\n",
       "timestamp                                                                \n",
       "2019-04-16 19:34:42   97.105.19.58  GET /api/v1/sales HTTP/1.1     200   \n",
       "2019-04-16 19:34:42   97.105.19.58  GET /api/v1/items HTTP/1.1     200   \n",
       "2019-04-16 19:34:44   97.105.19.58  GET /api/v1/sales HTTP/1.1     200   \n",
       "2019-04-16 19:34:46   97.105.19.58  GET /api/v1/sales HTTP/1.1     200   \n",
       "2019-04-16 19:34:48   97.105.19.58  GET /api/v1/sales HTTP/1.1     200   \n",
       "2019-04-17 12:55:14   97.105.19.58  GET /api/v1/sales HTTP/1.1     200   \n",
       "2019-04-21 10:02:41   95.31.18.119  GET /api/v1/items/HTTP/1.1     200   \n",
       "2019-04-17 19:36:41   95.31.16.121  GET /api/v1/sales/HTTP/1.1     301   \n",
       "2019-04-18 19:42:41  97.105.15.120  GET /api/v1/sales/HTTP/1.1     301   \n",
       "2019-04-19 19:42:41   97.105.19.58  GET /api/v1/sales/HTTP/1.1     200   \n",
       "\n",
       "                        size destination           request_agent   size_mb  \n",
       "timestamp                                                                   \n",
       "2019-04-16 19:34:42   512495         NaN  python-requests/2.21.0  0.488753  \n",
       "2019-04-16 19:34:42     3561         NaN  python-requests/2.21.0  0.003396  \n",
       "2019-04-16 19:34:44   510103         NaN  python-requests/2.21.0  0.486472  \n",
       "2019-04-16 19:34:46   510003         NaN  python-requests/2.21.0  0.486377  \n",
       "2019-04-16 19:34:48   511963         NaN  python-requests/2.21.0  0.488246  \n",
       "2019-04-17 12:55:14   510166         NaN  python-requests/2.21.0  0.486532  \n",
       "2019-04-21 10:02:41  1153005         NaN  python-requests/2.21.0  1.099591  \n",
       "2019-04-17 19:36:41     1005         NaN  python-requests/2.21.0  0.000958  \n",
       "2019-04-18 19:42:41     2560         NaN  python-requests/2.21.0  0.002441  \n",
       "2019-04-19 19:42:41  2056327         NaN  python-requests/2.21.0  1.961066  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().append(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 13978 entries, 2019-04-16 19:34:42 to 2019-04-19 19:42:41\n",
      "Data columns (total 7 columns):\n",
      "ip                13978 non-null object\n",
      "request_method    13978 non-null object\n",
      "status            13978 non-null int64\n",
      "size              13978 non-null int64\n",
      "destination       25 non-null object\n",
      "request_agent     13978 non-null object\n",
      "size_mb           13978 non-null float64\n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 873.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13978, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ip                    0\n",
       "request_method        0\n",
       "status                0\n",
       "size                  0\n",
       "destination       13953\n",
       "request_agent         0\n",
       "size_mb               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxcount:  5\n",
      "GET /api/v1/sales HTTP/1.1\n",
      "maxcount:  6\n",
      "GET /api/v1/items/next_page HTTP/1.1\n",
      "maxcount:  8\n",
      "GET /api/v1//api/v1/items HTTP/1.1\n",
      "maxcount:  9\n",
      "GET /api/v1//api/v1/items/next_page HTTP/1.1\n",
      "final maxcount is  9\n"
     ]
    }
   ],
   "source": [
    "maxcount = 0\n",
    "for i, command in enumerate(df.request_method):\n",
    "    if len(command.split('/')) > maxcount:\n",
    "        maxcount = len(command.split('/'))\n",
    "        print('maxcount: ', maxcount)\n",
    "        print(command)\n",
    "print('final maxcount is ', maxcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ip                 22\n",
       "request_method     22\n",
       "status              3\n",
       "size              191\n",
       "destination        18\n",
       "request_agent       9\n",
       "size_mb           191\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    13960\n",
       "499       16\n",
       "301        2\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.105.19.58      11999\n",
       "173.173.113.51     1059\n",
       "72.181.113.170      613\n",
       "72.181.105.81       246\n",
       "68.201.219.223       21\n",
       "24.26.242.9          21\n",
       "35.175.171.137        2\n",
       "70.121.214.34         2\n",
       "52.87.230.102         2\n",
       "52.90.165.200         1\n",
       "97.105.15.120         1\n",
       "34.207.64.242         1\n",
       "95.31.16.121          1\n",
       "54.145.52.184         1\n",
       "34.229.70.250         1\n",
       "95.31.18.119          1\n",
       "45.23.250.16          1\n",
       "35.174.209.2          1\n",
       "52.91.30.150          1\n",
       "3.92.201.136          1\n",
       "3.88.129.158          1\n",
       "54.172.14.223         1\n",
       "Name: ip, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ip.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('ip')['size_mb'].agg(['max', 'mean']).sort_values(by='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df = pd.DataFrame(df.groupby('ip')['size_mb']\\\n",
    "                     .mean().reset_index()\\\n",
    "                     .rename(index=str, columns={'index': 'ip','size_mb': 'avg_size_mb' }))\n",
    "ip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df2 = pd.DataFrame(df.ip.value_counts(dropna=False)/df.ip.count()).reset_index().\\\n",
    "                rename(index=str, columns={'index': 'ip', 'ip': 'ip_proba'})\n",
    "ip_df = ip_df.merge(ip_df2)\n",
    "\n",
    "\n",
    "# see those where rate < 1% \n",
    "ip_df[ip_df.ip_proba < .01].sort_values(by=['ip_proba', 'avg_size_mb'])\n",
    "# 19\t95.31.18.119\t1.099591\t0.000072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the request path into sections. Each section will get its own row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df['request_path'].str.split('/').apply(pd.Series, 1).stack()\n",
    "\n",
    "s.index = s.index.droplevel(-1) # to line up with df's index\n",
    "\n",
    "s.name = 'request_path' # needs a name to join\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the 'request_path' column before the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_request_path = df.drop(columns = 'request_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = df_no_request_path.join(s.apply(lambda x: pd.Series(x.split('/'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.rename(columns={0:'request_path_section'}, inplace=True)\n",
    "df_split.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But... that doesn't immediately appear to be useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size_mb'] = [n/1024/1024 for n in df['size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried to use a function, ipaddress.ip_address, that is supposed to return the country and other info about the ip address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipaddress import ip_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_address(df.ip)\n",
    "# I have no idea what this error means..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "try:\n",
    "    network = ip_address(df.ip)\n",
    "except ValueError:\n",
    "    count += 1\n",
    "    print('address/netmask is invalid:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ip.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new dataframe with only ip_address and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df['ip'].value_counts().reset_index()\n",
    "dff.columns = ['ip', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I'm using ipinfo to try to read the ipaddress. It works when I use the default that tells me the stats on MY ip address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = 'http://ipinfo.io/json'\n",
    "response = urlopen(url)\n",
    "data = json.load(response)\n",
    "\n",
    "IP=data['ip']\n",
    "org=data['org']\n",
    "city = data['city']\n",
    "country=data['country']\n",
    "region=data['region']\n",
    "\n",
    "print('Your IP detail\\n ')\n",
    "print('IP : {4} \\nRegion : {1} \\nCountry : {2} \\nCity : {3} \\nOrg : {0}'.format(org,region,country,city,IP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't figure out how to pass the ipaddress as an argument, so I found this code, but couldn't figure out how to make it work... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipinfo\n",
    "access_token = '123456789abc'\n",
    "handler = ipinfo.getHandler(access_token)\n",
    "ip_address = '173.173.113.51'\n",
    "details = handler.getDetails(ip_address)\n",
    "details.city\n",
    "details.loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Anomalies in Discrete Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding anomalies in already existing data\n",
    "\n",
    "We can see easily some anomalies around IP addresses --- that's from the curriculum... I'm not seeing any anomalies..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe with probabilities that each ip address will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df = pd.DataFrame(df.ip.value_counts(dropna=False)).reset_index().\\\n",
    "                rename(index=str, columns={'index': 'ip', 'ip': 'ip_count'})\n",
    "ip_df2 = pd.DataFrame(df.ip.value_counts(dropna=False)/df.ip.count()).reset_index().\\\n",
    "                rename(index=str, columns={'index': 'ip', 'ip': 'ip_proba'})\n",
    "ip_df = ip_df.merge(ip_df2)\n",
    "\n",
    "\n",
    "# see those where rate < 1% \n",
    "ip_df[ip_df.ip_proba < .01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only interested in the low probability ip addresses. Had to weed down to ip_df.ip_proba < 0.000005 to weed down the count significantly. That gave 256 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My graph is very pretty, but not at all meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_chance = ip_df[ip_df.ip_proba < 0.0002]\n",
    "sm_chance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sm_chance))\n",
    "\n",
    "print(sm_chance)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "splot = sns.barplot(data=sm_chance, x = 'ip', y = 'ip_count', ci = None)\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.0f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', xytext = (0, 10), \n",
    "                   textcoords = 'offset points'\n",
    "                   )\n",
    "    plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting anomalies by establishing a baseline and evaluate as new data arrives\n",
    "##### Establish baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping date and time and selecting a range of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df['2018-01-26 09:55:03':'2019-01-26 09:55:03'][['stuff', 'id', 'cohort', 'ip']]\n",
    "train = df[['ip', 'request_path', 'status', 'size', 'request_agent', 'size_mb']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute probabilities based on train sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df = pd.DataFrame(train.ip.value_counts(dropna=False)/train.ip.count()).reset_index().\\\n",
    "                rename(index=str, columns={'index': 'ip', 'ip': 'ip_proba'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge probabilities with all data (train + new data)\n",
    "- Where the ip address is new, i.e. not seen in the training dataset, fill the probability with a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().merge(ip_df, on=['ip'], how='left').fillna(value=0).set_index('timestamp')\n",
    "df.ip.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probabilities: probabilities using 2 discrete variables\n",
    "##### Probability of Status given IP Address:\n",
    "If we are looking for an unexpected status (like authentication failure) from a known/common IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_probs = train.groupby('ip').size().div(len(df))\n",
    "\n",
    "status_given_ip = pd.DataFrame(train.groupby(['ip', 'size_mb']).\\\n",
    "                               size().div(len(train)).\\\n",
    "                               div(ip_probs, \n",
    "                                   axis=0, \n",
    "                                   level='ip').\\\n",
    "                               reset_index().\\\n",
    "                               rename(index=str, \n",
    "                                      columns={0: 'proba_status_given_ip'})\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_status_count = pd.DataFrame(train.groupby(['ip', 'size_mb'])['request_path'].\\\n",
    "                                count().reset_index().\\\n",
    "                                rename(index=str, \n",
    "                                       columns={'request_path': 'ip_status_count'}))\n",
    "\n",
    "\n",
    "ip_status = status_given_ip.merge(ip_status_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add these probabilities to original events to detect anomalous events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().merge(ip_status, on=['ip', 'size_mb'], how='left').fillna(value=0).set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['proba_status_given_ip'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['proba_status_given_ip'] < .005).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_proba = df[df.proba_status_given_ip < .005]\n",
    "print(sm_proba.shape)\n",
    "sm_proba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.proba_status_given_ip, df.ip_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sm_proba.proba_status_given_ip, sm_proba.ip_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Anomalies of Continuous Variables with Time Series Using Statistical Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Pick a cohort and run this time series analysis on it*\n",
    "## Time series + EMA\n",
    "### Discover users who are accessing our curriculum pages way beyond the end of their codeup time. What would the dataframe look like? Use time series method for detecting anomalies, like exponential moving average with %b.\n",
    "\n",
    "\n",
    "\n",
    "### Statistical Methods\n",
    "- Flag the data points that deviate from the expected, based on the statistical properties, such as mean, median, mode, and quantiles.\n",
    "\n",
    "- You could define an anomalous data point as one that deviates by a certain standard deviation from the mean.\n",
    "\n",
    "- You could use a simple or exponential moving average to smooth short-term fluctuations and highlight long-term ones.\n",
    "\n",
    "- This method is challenging with really noisy data.\n",
    "\n",
    "### Anomalies in the amount of data consumed over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from numpy import linspace, loadtxt, ones, convolve\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from random import randint\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "# style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "def evaluate(actual, predictions, output=True):\n",
    "    mse = metrics.mean_squared_error(actual, predictions)\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    if output:\n",
    "        print('MSE:  {}'.format(mse))\n",
    "        print('RMSE: {}'.format(rmse))\n",
    "    else:\n",
    "        return mse, rmse    \n",
    "\n",
    "def plot_and_eval(predictions, actual, metric_fmt='{:.2f}', linewidth=4):\n",
    "    if type(predictions) is not list:\n",
    "        predictions = [predictions]\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(train,label='Train')\n",
    "    plt.plot(test, label='Test')\n",
    "\n",
    "    for yhat in predictions:\n",
    "        mse, rmse = evaluate(actual, yhat, output=False)        \n",
    "        label = f'{yhat.name}'\n",
    "        if len(predictions) > 1:\n",
    "            label = f'{label} -- MSE: {metric_fmt} RMSE: {metric_fmt}'.format(mse, rmse)\n",
    "        plt.plot(yhat, label=label, linewidth=linewidth)\n",
    "\n",
    "    if len(predictions) == 1:\n",
    "        label = f'{label} -- MSE: {metric_fmt} RMSE: {metric_fmt}'.format(mse, rmse)\n",
    "        plt.title(label)\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()    \n",
    "    \n",
    "def get_data():\n",
    "    df = pd.read_csv('http://python.zach.lol/access.log',          \n",
    "                  engine='python',\n",
    "                  header=None,\n",
    "                  index_col=False,\n",
    "                  names=['ip', 'timestamp', 'request_path', 'status', 'size', 'request_agent'],\n",
    "                  sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n",
    "                  na_values='-',\n",
    "                  usecols=[0, 3, 4, 5, 6, 8],\n",
    "                  skip_blank_lines=True)\n",
    "\n",
    "    df.timestamp = df.timestamp.str.replace('[', '')\n",
    "    df.timestamp = df.timestamp.str.replace(']', '')\n",
    "    df.timestamp= pd.to_datetime(df.timestamp.str.replace(':', ' ', 1)) \n",
    "\n",
    "    df.request_path = df.request_path.str.replace('\"', '')\n",
    "\n",
    "    df = df.set_index('timestamp')\n",
    "    df = df.tz_localize('utc').tz_convert('America/Chicago')\n",
    "    \n",
    "    # Replace the request paths that have only / with home_page.\n",
    "    df.replace(regex=r'^/$', value='home_page', inplace=True)\n",
    "    \n",
    "    # Take off the page number stuff from the request paths...\n",
    "    df['request_path'] = df.request_path.str.replace(r'\\?page=[0-9]+', '', regex=True)\n",
    "\n",
    "    for col in ['request_path', 'request_agent', 'destination']:\n",
    "        df[col] = df[col].str.replace('\"', '')\n",
    "\n",
    "    \n",
    "    # add a field for mb size\n",
    "    df['size_mb'] = [n/1024/1024 for n in df['size']]\n",
    "     \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrangle Data\n",
    "##### Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['ip', 'timestamp', 'request_method', 'status', 'size',\n",
    "          'destination', 'request_agent']\n",
    "df_orig = pd.read_csv('http://python.zach.lol/access.log',          \n",
    "                 engine='python',\n",
    "                 header=None,\n",
    "                 index_col=False,\n",
    "                 names=colnames,\n",
    "                 sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n",
    "                 na_values='\"-\"',\n",
    "                 usecols=[0, 3, 4, 5, 6, 7, 8]\n",
    ")\n",
    "\n",
    "new = pd.DataFrame([[\"95.31.18.119\", \"[21/Apr/2019:10:02:41+0000]\", \n",
    "                     \"GET /api/v1/items/HTTP/1.1\", 200, 1153005, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"95.31.16.121\", \"[17/Apr/2019:19:36:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 301, 1005, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"97.105.15.120\", \"[18/Apr/2019:19:42:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 301, 2560, np.nan, \n",
    "                     \"python-requests/2.21.0\"],\n",
    "                    [\"97.105.19.58\", \"[19/Apr/2019:19:42:41+0000]\", \n",
    "                     \"GET /api/v1/sales?page=79/HTTP/1.1\", 200, 2056327, np.nan, \n",
    "                     \"python-requests/2.21.0\"]], columns=colnames)\n",
    "\n",
    "df = df_orig.append(new)\n",
    "df.timestamp = df.timestamp.str.replace(r'(\\[|\\])', '', regex=True)\n",
    "df.timestamp= pd.to_datetime(df.timestamp.str.replace(':', ' ', 1)) \n",
    "df = df.set_index('timestamp')\n",
    "for col in ['request_method', 'request_agent', 'destination']:\n",
    "    df[col] = df[col].str.replace('\"', '')\n",
    "\n",
    "df['request_method'] = df.request_method.str.replace(r'\\?page=[0-9]+', '', regex=True)\n",
    "\n",
    "df['size_mb'] = [n/1024/1024 for n in df['size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df = pd.read_csv('http://python.zach.lol/access.log',          \n",
    "                  engine='python',\n",
    "                  header=None,\n",
    "                  index_col=False,\n",
    "                  names=['ip', 'timestamp', 'request_path', 'status', 'size', 'request_agent'],\n",
    "                  sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n",
    "                  na_values='-',\n",
    "                  usecols=[0, 3, 4, 5, 6, 8])\n",
    "\n",
    "    df.timestamp = df.timestamp.str.replace('[', '')\n",
    "    df.timestamp = df.timestamp.str.replace(']', '')\n",
    "    df.timestamp= pd.to_datetime(df.timestamp.str.replace(':', ' ', 1)) \n",
    "\n",
    "    df.request_path = df.request_path.str.replace('\"', '')\n",
    "\n",
    "    df = df.set_index('timestamp')\n",
    "    df = df.tz_localize('utc').tz_convert('America/Chicago')\n",
    "    \n",
    "    # Replace the request paths that have only / with home_page.\n",
    "    df.replace(regex=r'^/$', value='home_page', inplace=True)\n",
    "    \n",
    "    # Take off the page number stuff from the request paths...\n",
    "    df['request_path'] = df.request_path.str.replace(r'\\?page=[0-9]+', '', regex=True)\n",
    "    \n",
    "    # add a field for mb size\n",
    "    df['size_mb'] = [n/1024/1024 for n in df['size']]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcount = 0\n",
    "for i, command in enumerate(df.request_method):\n",
    "    if len(command.split('/')) > maxcount:\n",
    "        maxcount = len(command.split('/'))\n",
    "        print('maxcount: ', maxcount)\n",
    "        print(command)\n",
    "print('final maxcount is ', maxcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. resample to 30 minute intervals taking max of size\n",
    "2. fill in missing datetimestamps (those not present because no data was captured during that time. We want to have continuous time and those time periods filled with 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_datetime_fmt = mdates.DateFormatter('%m-%d %H:%T')\n",
    "\n",
    "df_ts_size = df['size_mb'].resample('30T').median()\n",
    "\n",
    "idx = pd.date_range(\n",
    "    df_ts_size.sort_index().index.min(), \n",
    "    df_ts_size.sort_index().index.max(),\n",
    "    freq='30min'\n",
    "    )\n",
    "\n",
    "df_ts_size = df_ts_size.reindex(idx, fill_value=0).fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using all data and not splitting into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_train = df_ts_size.head(1).index[0]\n",
    "end_date_train = '2019-04-17 23:30:00'\n",
    "start_date_test = '2019-04-18 00:00:00'\n",
    "\n",
    "train = df_ts_size[:end_date_train]\n",
    "test = df_ts_size[start_date_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head().append(train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train)\n",
    "plt.plot(test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMA - Simple Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the short-window simple moving average\n",
    "short_rolling = train.rolling(window=12).mean()\n",
    "\n",
    "# Calculating the long-window simple moving average\n",
    "long_rolling = train.rolling(window=24).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the 2 window sizes for the SMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "\n",
    "ax.plot(train.index, \n",
    "        train,\n",
    "        label='Size (MB)')\n",
    "\n",
    "ax.plot(short_rolling.index, \n",
    "        short_rolling, \n",
    "        label = '6-Hour SMA')\n",
    "ax.plot(long_rolling.index, \n",
    "        long_rolling, \n",
    "        label = '12-Hour SMA')\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "# ax.xaxis.(rotate=90)\n",
    "# ax.xaxis.set_major_formatter(my_datetime_fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the Exponential Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pandas to calculate a 2 hour span EMA. \n",
    "# adjust=False specifies that we are interested in the \n",
    "# recursive calculation mode.\n",
    "ema_short = train.ewm(span=12, adjust=False).mean()\n",
    "ema_short[0:3]\n",
    "\n",
    "ema_long = train.ewm(span=24, adjust=False).mean()\n",
    "ema_long[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare SMA with EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "\n",
    "ax.plot(train.index, \n",
    "        train,\n",
    "        label='Size (MB)')\n",
    "\n",
    "ax.plot(short_rolling.index, \n",
    "        short_rolling, \n",
    "        label = '6-Hour SMA')\n",
    "ax.plot(long_rolling.index, \n",
    "        ema_short, \n",
    "        label = 'Span 6-Hour EMA')\n",
    "ax.plot(long_rolling.index, \n",
    "        long_rolling, \n",
    "        label = '12-Hour SMA')\n",
    "ax.plot(long_rolling.index, \n",
    "        ema_long, \n",
    "        label = 'Span 12-Hour EMA')\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "\n",
    "yhat = pd.DataFrame(dict(actual=test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecast using the EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periods = 24\n",
    "yhat['moving_avg_forecast'] = ema_long.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the '%b' for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the absolute error:\n",
    "yhat['error'] = abs(yhat.actual - yhat.moving_avg_forecast)\n",
    "\n",
    "# compute the mean of the absolute error:\n",
    "# yhat.error.median()\n",
    "\n",
    "# compute upper band and lower band using IQR with weight of 3\n",
    "\n",
    "q3 = yhat.error.describe().loc['75%']\n",
    "q1 = yhat.error.describe().loc['25%']\n",
    "\n",
    "# adding .1 to the IQR so the we don't end up with a denominator of 0. \n",
    "ub = q3 + 3*(q3-q1+.1)\n",
    "lb = q1 - 3*(q3-q1+.1)\n",
    "\n",
    "yhat['pct_b'] = (yhat.actual-lb)/(ub-lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maggie's code to find outliers:\n",
    "# span = 24\n",
    "# ema_long = train.ewm(span=span, adjust=False).mean()\n",
    "# midband = ema_long[-1]\n",
    "# ub = midband + ema_long[-24:-1].std()*3\n",
    "# lb = midband - ema_long[-24:-1].std()*3\n",
    "\n",
    "# yhat['moving_avg_forecast'] = midband"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat[yhat.pct_b > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_eval(yhat.moving_avg_forecast, actual=test)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(yhat.pct_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat[yhat.pct_b > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.size_mb>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IP Address\tCountry\tRegion\tCity\n",
    "95.31.18.119\tRussia \tMoscow\tMoscow\n",
    "ISP\tOrganization\tLatitude\tLongitude\n",
    "CORBINA-BROADBAND\tNot Available\t55.7315\t37.6454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IP Address\tCountry\tRegion\tCity\n",
    "97.105.19.58\tUnited States \tTexas\tSan Antonio (Downtown)\n",
    "ISP\tOrganization\tLatitude\tLongitude\n",
    "Spectrum\tCodeup LLC\t29.4267\t-98.4896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
